{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJMd6UFPMhFfJxYF7VSJie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emy05/NLP-Tasks-Data-Analytics/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTENCE TOKENIZATION"
      ],
      "metadata": {
        "id": "rl78i-E7A8Lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzDVFN842zaH"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text to be analyzed\n",
        "text = \"Space exploration has captivated human curiosity for centuries, beginning with ancient astronomers observing the night sky and progressing to modern-day missions to the International Space Station and beyond.In recent years, there has been a renewed interest in space exploration, particularly with the rise of private companies like SpaceX and Blue Origin. These companies have been developing reusable rockets and other technologies to make space travel more affordable and accessible.The future of space exploration is also shaped by global collaboration, with the International Space Station being a prime example of this. NASA has plans to return to the moon with the Artemis program, which aims to establish a sustainable presence on the lunar surface by 2024. There are also plans for crewed missions to Mars in the coming decades, with NASA's Mars 2020 mission already underway.\"\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "sfCFBMNg389I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD TOKENIZATION"
      ],
      "metadata": {
        "id": "lGUAEF1sBU1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform word tokenization on each sentence\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "jlkexYCg4vJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for sentence in sentences:\n",
        "    sentence_words = word_tokenize(sentence)\n",
        "    words.append(sentence_words)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "u78Q64Ym43mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVING STOP WORDS"
      ],
      "metadata": {
        "id": "xuICQIaCBYS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words from each sentence\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "-qAuc7Bv48TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = []\n",
        "for sentence_words in words:\n",
        "    filtered_sentence_words = [word for word in sentence_words if word.lower() not in stop_words]\n",
        "    filtered_words.append(filtered_sentence_words)\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "id": "1XdFjZZY5Fdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "jstRlOv2BfcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stemming on each word\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "IRrCF_Hu5I5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = []\n",
        "for sentence_words in filtered_words:\n",
        "    stemmed_sentence_words = [stemmer.stem(word) for word in sentence_words]\n",
        "    stemmed_words.append(stemmed_sentence_words)\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "jVTDqjcv5Oq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "hpCWkqN9Bj7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform lemmatization on each word\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "TwTm3-Cm5zG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = []\n",
        "for sentence_words in filtered_words:\n",
        "    lemmatized_sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    lemmatized_words.append(lemmatized_sentence_words)\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "id": "dveOBtX753qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPENDENCY PARSING"
      ],
      "metadata": {
        "id": "SV55KISzBm1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "x1HcO_ZY8t-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "pyrPi8q--Dia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = text\n",
        "# nlp function returns an object with individual token information, \n",
        "# linguistic features and relationships\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(\"{:<15} | {:<8} | {:<15} | {:<20}\".format('Token','Relation','Head', 'Children'))\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for token in doc:\n",
        "    # Print the token, dependency nature, head and all dependents of the token\n",
        "    print(\"{:<15} | {:<8} | {:<15} | {:<20}\"\n",
        "          .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children])))\n",
        "\n",
        "# Use displacy to visualize the dependency \n",
        "displacy.render(doc, style='dep', options={'distance': 120})"
      ],
      "metadata": {
        "id": "CCfA2-wC-3W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERCEPTRON"
      ],
      "metadata": {
        "id": "MoGYymGoBrHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tag import PerceptronTagger\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "VElQMsUP_Hav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Perceptron tagger on the Brown corpus\n",
        "corpus = brown.tagged_sents(categories=\"news\")\n",
        "tagger = PerceptronTagger(load=False)\n",
        "tagger.train(corpus)\n",
        "\n",
        "# Tag a new sentence\n",
        "sentence = \"Space exploration has captivated human curiosity for centuries.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tags = tagger.tag(tokens)\n",
        "\n",
        "# Print the tagged sentence\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "30ylSipP_Ycm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER AND CHUNKING"
      ],
      "metadata": {
        "id": "xFtJ-D0mBuiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = text\n",
        "\n",
        "# nlp function returns an object with individual token information, \n",
        "# linguistic features and relationships\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract Named Entities\n",
        "print(\"{:<15} | {:<10}\".format('Entity', 'Label'))\n",
        "print(\"-\" * 25)\n",
        "for ent in doc.ents:\n",
        "    print(\"{:<15} | {:<10}\".format(ent.text, ent.label_))\n",
        "\n",
        "# Extract Noun Chunks\n",
        "print(\"\\nNoun chunks:\")\n",
        "print(\"-\" * 25)\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ],
      "metadata": {
        "id": "rYU5Hy_1AlSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}